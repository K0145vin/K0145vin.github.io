<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>[论文阅读] UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling</title>
    <link href="/2023/04/26/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-UniAdapter-Unified-Parameter-Efficient-Transfer-Learning-for-Cross-modal-Modeling/"/>
    <url>/2023/04/26/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-UniAdapter-Unified-Parameter-Efficient-Transfer-Learning-for-Cross-modal-Modeling/</url>
    
    <content type="html"><![CDATA[<h1 id="motivation">Motivation</h1><p>先前的工作都是考虑的单一模态或者单一下游任务，且没有考虑过跨模态之间的交互和知识共享。</p><p>优势： 1. 知识共享 2. 为保证语言查询在交叉注意力中的完整性，对语言查询进行了残差学习 3. 无参数的帧感知注意力，能够无成本地统一视频和图像模态，不仅适用于更多的下游任务，且能够缓解视频帧中的噪声问题</p><h1 id="methodology">Methodology</h1><p><img src="https://k0145vin.oss-cn-hangzhou.aliyuncs.com/Image202304261558303.png" /></p><h2 id="preliminary">Preliminary</h2><h3 id="vision-language-framework">Vision-language Framework</h3><p>用单模态编码器提取视觉特征 <span class="math inline">\(\mathbf{f}^v=\{f^v_{\text{CLS}},f^v_{0},f^v_{1},\dots\}\)</span>和文本特征 <span class="math inline">\(\mathbf{f}^t=\{f^t_{\text{CLS}},f^t_{0},f^t_{1},\dots\}\)</span>。然后，将提取的特征送入多模态编码器中。具体来说就是，将文本特征作为输入，将视觉特征插入到每个交叉注意力层中。</p><p>对于视频-语言任务，首先用视觉编码器提取每一帧的特征 <span class="math inline">\(\mathbf{f}^e=\{f^e_{\text{CLS}},f^e_{0},f^e_{1},\dots\}\)</span> ，然后连接起来作为输入。</p><h3 id="adapter">Adapter</h3><p>每个 adapter 包含一个下投射层 <span class="math inline">\(W_{down}\in\mathcal{R}^{(d\times r)}\)</span> ，一个非线性激活函数 <span class="math inline">\(\sigma\)</span> 和一个上投射层 <span class="math inline">\(W_{up}\in\mathcal{R}^{(r\times d)}\)</span> 。Adapter 的计算过程如下： <span class="math display">\[Apater(x)=x+s\cdot\sigma(xW_{down})W_{up}\]</span> 其中，<span class="math inline">\(s\)</span> 是缩放因子。</p><h2 id="residual-leaning-for-language-queries">Residual Leaning for Language Queries</h2><p>常规的方法是在编码器的多头注意力模块后直接插入 adapter ，但是这种做法很难处理混合信息，而且有可能在交叉注意力处理过程中破坏语言查询的完整性，因而提出用于语言查询的残差学习。</p><p>每个多模态编码器包含一个多头自注意力，一个多头交叉注意力和一个 FFN 。多模态编码器将文本特征作为输入，将视觉信息注入到每个交叉注意力层中。每个交叉注意力层将自注意力的输出特征 <span class="math inline">\(\mathbf{q}\)</span> 作为 <span class="math inline">\(Q\)</span> ，视觉特征 <span class="math inline">\(\mathbf{f}^v\)</span> 作为 <span class="math inline">\(K\)</span> 和 <span class="math inline">\(V\)</span> 。计算过程如下： <span class="math display">\[\begin{array}{l}\boldsymbol{q}=\boldsymbol{l}_{\boldsymbol{l}-\boldsymbol{1}}+\operatorname{MSA}\left(\boldsymbol{l}_{\boldsymbol{l}-\mathbf{1}}\right) \\\boldsymbol{h}=\boldsymbol{q}+\operatorname{MCA}\left(Q=\boldsymbol{q}, K=\mathbf{f}^{\boldsymbol{v}}, V=\mathbf{f}^{\boldsymbol{v}}\right) \\\boldsymbol{l}_{\boldsymbol{l}}=\boldsymbol{h}+\operatorname{FFN}(\boldsymbol{h})\end{array}\]</span> 其中，<span class="math inline">\(\boldsymbol{l}_0=\mathbf{f}^t\)</span> ，<span class="math inline">\(\boldsymbol{l_l}\)</span> 代表第 <span class="math inline">\(l\)</span> 层的输出特征。在交叉注意力层之后插入标准的 adapter ： <span class="math display">\[\boldsymbol{l_l}=Adapter(\boldsymbol{h})+\operatorname{FFN}(\operatorname{LN}(\boldsymbol{h}))\]</span> 隐藏层 <span class="math inline">\(\boldsymbol{h}\)</span> 包含了查询特征和跨模态融合特征。对于一个单模态 adapter 来说学习这些混合信息是非常困难的。而且，文本查询信息在交叉注意力之间传输时还有可能丢失。所以，引入了一个额外的残差形式的 adapter 来维护查询信息。具体来说，在自注意力层后插入 adapter，直接以残差的形式将输出加到前馈层。上式重写为： <span class="math display">\[\boldsymbol{l_l}=Adapter(\boldsymbol{q})+Adapter(\boldsymbol{h})+\operatorname{FFN}(\operatorname{LN}(\boldsymbol{h}))\]</span> 简单引入查询残差 adapter 就会引入额外的更新参数，这与轻量化的原则不符。而文本编码器也会把文本特征作为输入，并把输出加到前馈层。因此，文本 adapter 的知识可以共享权重的方式与查询残差 adapter 共享来避免额外的更新参数。而且共享权重机制还会带来更好的效果。</p><p><img src="https://k0145vin.oss-cn-hangzhou.aliyuncs.com/Image202304261934989.png" /></p><h2 id="uniadapter">UniAdapter</h2><p>将视觉-语言模型迁移到下游任务中，一个最直接的方法就是为每一个模态模块使用 adapter，但是这样会带来较高的参数。而且，这些 adapter 之间没有跨模态交互，进而导致性能不好。UniAdapter通过共享部分权重的方式将单模态、多模态的 adapter 统一到一个框架下。</p><p>UniAdapter 的核心思想就是共享多种模态的知识来增强跨模态交互，同时减少参数量。UniAdapter 包括一个统一的下投射层 <span class="math inline">\(W_{down}\in\mathcal{R}^{(d\times r)}\)</span> ，一个非线性激活函数 <span class="math inline">\(\sigma\)</span> 和一个针对特定模态的上投射层 <span class="math inline">\(W_{up}^{\mathcal{M}}\in\mathcal{R}^{(r\times d)},\mathcal{M}\in\{\mathcal{V},\mathcal{T},\mathcal{C}\}\)</span> ，其中 <span class="math inline">\(\mathcal{V},\mathcal{T},\mathcal{C}\)</span> 分别代表视觉、文本和跨模态。UniAdapter中所有的下投射层都是共享的，而上投射层是针对特定模态进行学习。</p><h3 id="unimodal-case">Unimodal Case</h3><p>尽管使用了统一的下投射层来进行跨模态知识共享，但学习特定模态的表征对于单模态编码器也很重要。所以，使用了特定模态的上投射层 <span class="math inline">\((W_{up}^{\mathcal{V}}, W_{up}^{\mathcal{T}})\)</span> 分别用于视觉和文本编码器： <span class="math display">\[\begin{array}{l}UniAdapter(x^{\mathcal{V}})=x^{\mathcal{V}}+s\cdot\sigma(x^{\mathcal{V}}W_{down})W_{up}^{\mathcal{V}},\\UniAdapter(x^{\mathcal{T}})=x^{\mathcal{T}}+s\cdot\sigma(x^{\mathcal{T}}W_{down})W_{up}^{\mathcal{T}},\end{array}\]</span> 其中，<span class="math inline">\(s\)</span> 是缩放因子，<span class="math inline">\(x^{\mathcal{V}},x^{\mathcal{T}}\)</span> 分别地标视觉和文本特征。</p><p>视觉和文本编码器使用相同的 transformer encoder 结构，遵循 MAM 的设置将 UniAdapter 放在 自注意力层和前馈层之间。</p><h3 id="cross-modal-case">Cross-modal Case</h3><p>此外还利用一个特定的上投射层进行多模态编码器的迁移学习。但是输入特征包含了查询特征和跨模态融合特征。用单个 adapter 来学习这写混合信息非常困难。因此考虑上文提到的，在 UniAdapter 中重复利用文本上投射层 <span class="math inline">\(W_{up}^{\mathcal{T}}\)</span> 来捕获文本信息。这样一来，跨模态上投射层 <span class="math inline">\(W_{up}^{\mathcal{C}}\)</span> 可以更容易地处理跨模态信息。跨模态的 UniAdapter可以表示为： <span class="math display">\[\begin{aligned}\operatorname{UniAdapter}\left(x^{\mathcal{C}}\right)= &amp; x^{\mathcal{C}}+s \cdot\left[\sigma\left(x^{\mathcal{C}} W_{\text {down }}\right) W_{u p}^{\mathcal{T}}\right. \\&amp; \left.+\sigma\left(x^{\mathcal{C}} W_{\text {down }}\right) W_{u p}^{\mathcal{C}}\right]\end{aligned}\]</span> 对于多模态编码器，在交叉注意力层和前馈层之间插入 UniAdapter 。 <span class="math display">\[\boldsymbol{l_l}=UniAdapter(\boldsymbol{q})+UniAdapter(\boldsymbol{h})+\operatorname{FFN}(\operatorname{LN}(\boldsymbol{h}))\]</span> ## Parameter-free Frame-aware Attention 对于给定的视频文本对，所提取的帧特征为 <span class="math inline">\(\{f^e_{\text{CLS},i},f^e_{i,j}|i=1,\dots,n,j=1,\dots,m\}\)</span> ，<span class="math inline">\(n\)</span> 为视频长度，<span class="math inline">\(m\)</span> 为 token 长度。定义 <span class="math inline">\(A_i\)</span> 为帧特征和匹配的文本 token特征做点积所得到的第 <span class="math inline">\(i\)</span> 帧的注意力权重： <span class="math display">\[A_i=\frac{\exp(f^e_{\text{CLS},i}\cdot f_{\text{CLS}}^t)}{\sum_i\exp(f^e_{\text{CLS},i}\cdot f_{\text{CLS}}^t)}\]</span> 对每一帧特征 <span class="math inline">\(\mathbf{f}^e\)</span> 应用 PFA 注意力权重来计算最终输入的视觉特征： <span class="math display">\[PFA(\mathbf{f}^e)=\{f^e_{\text{CLS},i},A_i*f^e_{i,j}|1\leq i\leq n,1\leq j \leq m\}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>PEFT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[论文阅读] LANGUAGE MODELLING WITH PIXELS</title>
    <link href="/2023/04/26/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-LANGUAGE-MODELLING-WITH-PIXELS/"/>
    <url>/2023/04/26/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-LANGUAGE-MODELLING-WITH-PIXELS/</url>
    
    <content type="html"><![CDATA[<h1 id="approach">Approach</h1><p><img src="https://k0145vin.oss-cn-hangzhou.aliyuncs.com/Image202304172125362.png" /></p><p>三个主要组件： * 文本渲染器：把文字处理成图片 * 编码器：对图片的未遮盖区域进行编码 * 解码器：重建遮盖区域像素</p><h2 id="text-renderer">TEXT RENDERER</h2><p><img src="https://k0145vin.oss-cn-hangzhou.aliyuncs.com/Image202304172141234.png" /></p><p>把文本渲染成 RGB 图片，<span class="math inline">\(x\in\mathbb{R}^{H\times W\times C},H=16,W=8464,C=3\)</span> ，这样就跟分辨率为 <span class="math inline">\(384\times384\)</span> 的图片大小一致，可以分为 <span class="math inline">\(529\)</span> 个 <span class="math inline">\(16\times16\)</span> 的 patch 。序列末尾的空白 patch 不会计算注意力或者损失。 超长的序列会被缩短或者分成多个序列。</p><h2 id="architecture">ARCHITECTURE</h2><h3 id="patch-span-masking">Patch Span Masking</h3><p><img src="https://k0145vin.oss-cn-hangzhou.aliyuncs.com/Image202304172153602.png" /></p><p>不同于 ViT-MAE 的随机屏蔽或者 BEiT 中块级别的屏蔽，PIXEL 采用 <span class="math inline">\(25\%\)</span> 的屏蔽率，最多屏蔽连续 6 个 patch，中间留有不定数量未屏蔽的patch。</p><h1 id="finetuning">FINETUNING</h1><h2 id="extractive-question-answeringqa">Extractive Question Answering(QA)</h2><p>使用滑动窗口的方法来提取超过最大序列长度的例子的答案</p><p>使用一个线性分类器来预测包含答案的间断的开始和结束的 patch</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[论文阅读] AIM: ADAPTING IMAGE MODELS FOR EFFICIENT VIDEO ACTION RECOGNITION</title>
    <link href="/2023/03/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-AIM-ADAPTING-IMAGE-MODELS-FOR-EFFICIENT-VIDEO-ACTION-RECOGNITION/"/>
    <url>/2023/03/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-AIM-ADAPTING-IMAGE-MODELS-FOR-EFFICIENT-VIDEO-ACTION-RECOGNITION/</url>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/2302.03024">AIM: ADAPTING IMAGE MODELS FOR EFFICIENT VIDEO ACTION RECOGNITION</a></p><h1 id="introduction">Introduction</h1><p><img src="https://k0145vin.oss-cn-hangzhou.aliyuncs.com/Image202303272201795.png" /></p><p>在视频理解领域，目前主流的两种做法： 1. 在Image Model上加Temporal Module 2. 将Image Model扩展成Video Model</p><p>这些做法存在明显的缺点： 1. 需要full fine-tuning，耗费计算资源 2. 预训练的图像模型具有出色的可移植性，是否还有必要进行full fine-tuning？不充分的fine-tuning可能会破坏模型良好的泛化能力，“灾难性遗忘”</p><p>image-to-image，video-to-video的PEFT工作很多，但是image-to-video的PEFT工作还比较少，因为image model缺少时序推理能力。</p><p>在这篇文章中，作者提出了AIM用于视频动作识别。将预训练的图像模型参数冻结，添加一些轻量化的<a href="http://proceedings.mlr.press/v97/houlsby19a.html">adapter</a>进行fine-tuning，可以以更少的参数量达到SOTA甚至更好的效果。</p><p><img src="https://k0145vin.oss-cn-hangzhou.aliyuncs.com/Image202303272216512.png" /></p><p>先添加了<em>spatial adaptation</em>，但是预训练的图像模型在视频理解中已经有足够好的空间建模能力了；复用了预训练图像模型的self-attention层，作用于时间维度，添加<em>temporal adaptation</em>；最后添加<em>joint adaptation</em>。</p><h1 id="methodology">Methodology</h1><h2 id="prelimnary">Prelimnary</h2><p>一般的每个transformer block由一个多头自注意力、一个MLP层、LN层和残差连接构成。写为：</p><p><span class="math display">\[z^{\prime}_l=z_{l-1} + \operatorname{MSA}(\operatorname{LN}(z_{l-1}))\]</span> <span class="math display">\[z_{l}=z_{l}^{\prime}+\operatorname{MLP}(\operatorname{LN}(z_{l}^{\prime}))\]</span> <img src="https://k0145vin.oss-cn-hangzhou.aliyuncs.com/Image202303282011349.png" /></p><h2 id="spatial-adapation">Spatial Adapation</h2><p>预训练模型已经在大规模数据集上进行了训练，对下游任务有着强大的迁移能力，作者相信它们在视频动作识别中只经过很小的改动就能有很好的空间建模能力。</p><p>Adapter包含了两个全连接层、一个激活函数和一个残差连接，第一个全连接层降维，第二个全连接层升维。为了将预训练的空间特征应用到视频数据中，作者在自注意力层之后加入了一个Adapter，称为<em>spatial adaptation</em>。在训练过程中，transformer层的参数都是冻结的，只有adpater的参数是更新的。</p><p><img src="https://k0145vin.oss-cn-hangzhou.aliyuncs.com/Image202303282055287.png" /></p><p>上表表明：adapter确实帮助了冻结的图像模型学习到了很好的空间信息，但是与full fine-tuned的模型的性能还有很大的差别，这是因为只有spatial adaptation缺少学习视频中时序信息的能力。</p><h2 id="temporal-adaptation">Temporal Adaptation</h2><p>先前的方法中为了获取时序信息都会给预训练的图像模型添加temporal modules。然而，添加新的temporal modules，无论是temporal attention还是temporal encoder/decoder，都会引入大量额外的可调节参数。而且，这些新的模块都需要full fine-tuning。</p><p>为解决这一问题，作者提出了一种新的策略：复用图像模型中预训练好的自注意力层来做时序建模。将原来的自注意力层称为S-MSA，用于空间建模；复用的自注意力层称为T-MSA，用于时序建模，把T-MSA放在S-MSA前面。给定video pathch embedding <span class="math inline">\(z\in\mathbb{R}^{T\times(N+1)\times D}\)</span>，先将它reshape为<span class="math inline">\(z^{T}\in\mathbb{R}^{(N+1)\times T\times D}\)</span>，送入T-MSA中来学习<span class="math inline">\(T\)</span>帧之间的关系。接下来，跟之前spatial adaptation一样，在复用的注意力层之后加入一个adpater，结构同spatial adaptation一样，<strong>但是没有残差连接</strong>。这样做是为了使模型接近原始模型，所以将adpater的参数初始化为0，并且删除了残差连接，<em>以移除训练开始时temporal adaptation的影响</em>（？）。Temporal adaptation在只引入了另一个轻量化的adapter后就取得了与full fine-tuned模型相近的性能。</p><h2 id="joint-adaptation">Joint Adaptation</h2><p>最后，引入了与MLP层平行的Adapter，称为<em>joint adaptation</em>。这个adapter跟temporal adaptation中的拥有相同的结构。</p><p>最终的计算过程可以写为：</p><p><span class="math display">\[z_{l}^{T}=z_{l-1}+\operatorname{Adapter}(\operatorname{T-MSA}(\operatorname{LN}(z_{l-1})))\]</span> <span class="math display">\[z_{l}^{S}=z_{l}^{T}+\operatorname{Adapter}(\operatorname{S-MSA}(\operatorname{LN}(z_{l}^{T})))\]</span> <span class="math display">\[z_{l}=z_{l}^{S}+\operatorname{MLP}(\operatorname{LN}(z_{l}^{S}))+s\cdot\operatorname{Adapter}(\operatorname{LN}(s_{l}^{S}))\]</span></p><h1 id="conclusion">Conclusion</h1><p>T-MSA只是简单复用了S-MSA，对于视频的时序建模能力不够强大。视频时序建模可以看成是一种序列建模的形式，因此以后可以考虑使用文本或者音频模型的预训练权重，而不是图像模型的。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>PEFT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[论文阅读]DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting</title>
    <link href="/2023/03/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-DenseCLIP-Language-Guided-Dense-Prediction-with-Context-Aware-Prompting/"/>
    <url>/2023/03/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-DenseCLIP-Language-Guided-Dense-Prediction-with-Context-Aware-Prompting/</url>
    
    <content type="html"><![CDATA[<h2 id="approach">Approach</h2><h3 id="context-aware-prompting">Context-Aware Prompting</h3><p><img src="https://k0145vin.oss-cn-hangzhou.aliyuncs.com/Image202303092131810.png" /></p><h4 id="language-domain-prompting">Language-domain prompting</h4><p>不再使用人工设计的模板作为文本提示，受CoOp的启发，使用可学习的文本上下文作为基线，只包含语言域的提示，则文本编码器的输入变为： <span class="math display">\[[\mathbf{p},\mathbf{e}_k],\quad 1\leq k\leq K,\]</span></p><p>其中，<span class="math inline">\(\mathbf{p}\in\mathbb{R}^{N\times C}\)</span>为可学习的文本上下文，<span class="math inline">\(\mathbf{e}_k\in\mathbb{R}^C\)</span>为第k类名称的嵌入。</p><h4 id="vision-to-language-prompting">Vision-to-language prompting</h4><p>包含视觉内容的描述可以是文本更加精确。因此，使用transformer decoder中的交叉注意力机制来建模视觉和语言之间的交互。 <img src="https://k0145vin.oss-cn-hangzhou.aliyuncs.com/Image202303091541826.png" /></p><p>有两种不同的上下文感知提示策略，正如Fig4中展示的。第一种策略称为<em>pre-model prompting</em>，把特征<span class="math inline">\([\bar{\mathbf{z}},\mathbf{z}]\)</span>传入transformer decoder中编码视觉上下文： <span class="math display">\[\mathbf{v}_{\operatorname{pre}}=\operatorname{TransDecoder}(\mathbf{q},[\bar{\mathbf{z}},\mathbf{z}])\]</span> 其中，<span class="math inline">\(\mathbf{q}\in\mathbb{R}^{N\times C}\)</span>是一个可学习query的集合，<span class="math inline">\(\mathbf{v}_{\operatorname{pre}}\in\mathbb{R}^{N\times C}\)</span>是提取出来的视觉上下文。用<span class="math inline">\(\mathbf{v}_{\operatorname{pre}}\)</span>替代上式中的<span class="math inline">\(\mathbf{p}\)</span>作为文本编码器的输入。</p><p>另外一种选择是在文本编码器之后细化文本特征，称为<em>post-model prompting</em>。使用CoOp生成文本特征，并直接作为transformer decoder的查询： <span class="math display">\[\mathbf{v}_{\operatorname{post}}=\operatorname{TransDecoder}(\mathbf{t},[\bar{\mathbf{z}},\mathbf{z}])\]</span> 这种方式使得文本可以寻找最相关的视觉线索，然后通过残差连接更新文本特征： <span class="math display">\[\mathbf{t}\leftarrow\mathbf{t}+\gamma\mathbf{v}_{\operatorname{post}}\]</span> 其中，<span class="math inline">\(\gamma\in\mathbb{R}^C\)</span>是个可学习参数，来控制残差连接的比例。</p><p>尽管两种版本的目标相同，但是文章作者更倾向于后提示，主要原因有两个：（1）后提示更高效。由于预提示的输入依赖于图像，因此在推理过程中需要文本编码器额外的前向通道。在后提示的情况下，可以在训练后保存提取的文本特征，从而减少推理过程中文本编码器带来的开销。（2）实验结果也表明，后提示的性能更好。</p><h3 id="instantiations">Instantiations</h3><h4 id="semantic-segmentation">Semantic segmentation</h4><p>整个架构是模型无关的，可以应用于任何稠密预测任务。提出使用一个辅助目标来更好地利用像素-文本得分图进行分割。由于得分图<span class="math inline">\(\mathbf{s}\in\mathbb{R}^{H_4W_4\times K}\)</span>可以看作一个小的分割结果，因此可以在上面计算一个分割损失： <span class="math display">\[\mathcal{L}^{\operatorname{seg}}_{\operatorname{aux}}=\operatorname{CrossEntropy}(\operatorname{Softmax}(\mathbf{s}/\tau),\mathbf{y})\]</span> 其中，<span class="math inline">\(\tau=0.07,\mathbf{y}\in\{1,\dots,K\}^{H_4W_4}\)</span>是真实值标签。辅助分割损失函数可以帮助特征图更快恢复其局部性，有利于稠密预测任务进行分割和检测。 #### Object detection &amp; instance segmentation</p><p>在这种情况下，没有真实值分割标签。为了构建类似于分割中的辅助损失，使用边界框和标签构建一个二进制目标<span class="math inline">\(\tilde{\mathbf{y}}\in\{0,1\}^{H_4W_4\times K}\)</span>。辅助目标可以定义为二元交叉熵损失： <span class="math display">\[\mathcal{L}^{\operatorname{det}}_{\operatorname{aux}}=\operatorname{BinaryCrossEntropy}(\operatorname{Sigmoid}(\mathbf{s}/\tau),\tilde{\mathbf{y}})\]</span></p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>CLIP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[论文阅读]Video Swin Transformer</title>
    <link href="/2023/03/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Video-Swin-Transformer/"/>
    <url>/2023/03/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Video-Swin-Transformer/</url>
    
    <content type="html"><![CDATA[<h2 id="overall-architecture">Overall Architecture</h2><p><img src="https://k0145vin.oss-cn-hangzhou.aliyuncs.com/Image202303081528967.png" /> Fig2展示了Video Swin Transformer的tiny版本。输入视频的大小为<span class="math inline">\(T\times H\times W \times 3\)</span>，共<span class="math inline">\(T\)</span>帧，每帧包含<span class="math inline">\(H\times W \times 3\)</span>个像素。在Video Swin Transformer中将大小为<span class="math inline">\(2\times4\times4\times3\)</span>的3D patch作为一个token，所以分隔层共包含<span class="math inline">\(\frac{T}{2}\times\frac{H}{4}\times\frac{W}{4}\)</span>个3D token，每个token中包含一个96维的特征。然后经过一个线性层将特征投影到任意维度<span class="math inline">\(C\)</span>。Patch合并层会合并相邻<span class="math inline">\(2\times2\)</span>的patch，并经过线性层连接起来，维度减至原来的一半。</p><h2 id="d-shifted-window-based-msa-module">3D Shifted Window based MSA Module</h2><h3 id="multi-head-self--attention-on-non-overlapping-3d-windows">Multi-head self- attention on non-overlapping 3D windows</h3><p>给定一个包含<span class="math inline">\(T^\prime\times H^\prime\times W^\prime\)</span>个3D token的视频，3D窗口大小为<span class="math inline">\(P\times M\times M\)</span>，则被分割为<span class="math inline">\(\lceil\frac{T^\prime}{P}\rceil\times\lceil\frac{H^\prime}{M}\rceil\times\lceil\frac{W^\prime}{M}\rceil\)</span>个互不重叠的3D窗口。</p><p><img src="https://k0145vin.oss-cn-hangzhou.aliyuncs.com/Image202303081554766.png" /></p><h3 id="d-shifted-windows">3D Shifted Windows</h3><p>由于多头注意力机制应用在每个不重叠的3D窗口内，不同窗口之间缺乏联系，这可能会限制架构的表示能力。因此，将Swin Transformer中滑动的2D窗口扩展到3D，引入跨窗口联系，同时保持给予自注意力的非重叠窗口的计算效率。</p><p>给定输入的3D token的数量为<span class="math inline">\(T^\prime\times H^\prime\times W^\prime\)</span>，每个3D窗口的大小为<span class="math inline">\(P\times M \times M\)</span>，对于连续的两层，第一层中的自注意力模块采用常规的窗口分割策略，得到<span class="math inline">\(\lceil\frac{T^\prime}{P}\rceil\times\lceil\frac{H^\prime}{M}\rceil\times\lceil\frac{W^\prime}{M}\rceil\)</span>个不重叠的3D窗口。对于第二层中的自注意力模块 ，窗口分割根据前一层的自注意力模块沿时间、高度和宽度坐标移动<span class="math inline">\((\frac{T}{2}\times\frac{H}{4}\times\frac{W}{4})\)</span>个token。</p><p>Fig3解释了这一过程。虽然窗口的数量增加了，但是根据Swin Transformer的设置，最终的计算窗口数量仍然是8.</p><p>采用滑动窗口的分割方法，计算两个连续的Video Swin Transformer块： <span class="math display">\[\begin{array}{l}\hat{\mathbf{z}}^l=\operatorname{3DW-MSA}(\operatorname{LN}(\mathbf{z}^{l-1}))+\mathbf{z}^{l-1},\\\mathbf{z}^l=\operatorname{FFN}(\operatorname{LN}(\hat{\mathbf{z}}^l))+\mathbf{z}^l,\\\hat{\mathbf{z}}^{l+1}=\operatorname{3DSW-MSA}(\operatorname{LN}(\mathbf{z}^{l}))+\mathbf{z}^{l},\\\mathbf{z}^{l+1}=\operatorname{FFN}(\operatorname{LN}(\hat{\mathbf{z}}^{l+1}))+\mathbf{z}^{l+1}. \end{array}\]</span> 其中<span class="math inline">\(\hat{\mathbf{z}}^l,\mathbf{z}^l\)</span>分别代表3D(S)W-MSA模块和FFN模块的输出特征。</p><h3 id="d-relative-position-bias">3D Relative Position Bias</h3><p>在自注意力头引入3D相对位置偏差<span class="math inline">\(B\in\mathbb{R}^{P^2\times M^2\times M^2}\)</span>： <span class="math display">\[\operatorname{Attention}(Q,K,V)=\operatorname{SoftMax}(QK^T/\sqrt{d}+B)V\]</span></p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[论文阅读]STAViS: Spatio-Temporal AudioVisual Saliency Network</title>
    <link href="/2023/03/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-STAViS-Spatio-Temporal-AudioVisual-Saliency-Network/"/>
    <url>/2023/03/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-STAViS-Spatio-Temporal-AudioVisual-Saliency-Network/</url>
    
    <content type="html"><![CDATA[<h2 id="spatio-temporal-audio-visual-saliency-network">Spatio-Temporal Audio Visual Saliency Network</h2><p><img src="https://k0145vin.oss-cn-hangzhou.aliyuncs.com/Image202303012155196.png" /> 本文提出的网络架构包含一个用于计算时空视觉显著性的模块、一个基于Sound-net的音频表征模块计算音频特征、一个声源定位模块来计算时空听觉显著性的模块，以及一个音视显著性模块来融合视觉和听觉显著性，并评估损失。 ### Spatio-Temporal Visual Network 视觉显著性部分，使用3D卷积用于行为分类，包含参数<span class="math inline">\(\mathbf{W}_{res}\)</span>，四个卷积块<span class="math inline">\(conv1,conv2,conv3,conv4\)</span>从不同的时空尺度上提供输出<span class="math inline">\(X^1,X^2,X^3,X^4\)</span>。同时，注意力机制DSAM(Deeply Supervised Attention Module)对特征图<span class="math inline">\(X^m\)</span>和注意力图<span class="math inline">\(M^m\)</span>的每个通道做乘积来增强特征表征中最突出的区域： <span class="math display">\[\tilde{X}^m=(1+M^m)\odot X^m,\quad m=1,\dots,4.\]</span> 深度监督是DSAM的核心思想，之前已经被应用于边缘检测、目标分割和静态显著性检测，但是与上述工作不同，这里DSAM的作用是双重的：既用于增强视觉特征，也用于提供多尺度的显著图，就如Fig2中深浅不一的绿色线标注的。因此，DSAM的参数<span class="math inline">\(\mathbf{W}_{am}^m\)</span>通过视觉网络的主路径和残差连接的眼球跟踪数据来训练。 <img src="https://k0145vin.oss-cn-hangzhou.aliyuncs.com/Image202303020741821.png" /> Fig3展示DSAM模块在第<span class="math inline">\(m\)</span>层的结构，包含了一个时间维度上的average pooling层，然后是两个空间卷积层，提供显著性特征<span class="math inline">\(S^m\)</span>和激活图(?)<span class="math inline">\(A^m\)</span>。两种表征都通过逆卷积层上采样至初始图像的大小，用于模块的深度监督和多尺度显著性建立。对激活图<span class="math inline">\(A^m(x,y)\)</span>进行空间softmax操作可以得到注意力图<span class="math inline">\(M^m(x,Y)\)</span>: <span class="math display">\[M^m(x,y)=\frac{\exp(A^m(x,y))}{\sum_x\sum_y\exp(A^m(x,y))}\]</span></p><h3 id="audio-representation-network">Audio Representation Network</h3><p>对于音频数据，直接在声波上应用一维的卷积。首先将音频分割以匹配视频帧数（16帧）。应用的网络可以处理变长的音频，因此不同视频间不需要采用下采样策略来改变采样率。随后，应用一个汉宁窗来提高代表当前时间实例的中心音频值的权重，但也包括过去和未来的衰减值。之后，对于高层信息编码，采用基于SoundNet前七层的网络结构，其参数位<span class="math inline">\(\mathbf{W}_a\)</span>。这些层之后是一个时间维度的max-pooling层，以获得对于整个序列一个固定的维度向量<span class="math inline">\(f_a\in\mathbb{R}^{D_a}\)</span>。</p><h3 id="sound-source-localization-in-videos">Sound Source Localization in Videos</h3><p>选择3D卷积块<span class="math inline">\(conv3\)</span>的输出<span class="math inline">\(X^3\)</span>（特征维度为<span class="math inline">\(D_v\)</span>）作为视觉特征，因为这一层中加油丰富的视觉流语义信息和空间域上相当大的分辨率。应用时间平均池化来边缘化时间维度，获得整个序列的全局表示<span class="math inline">\(f_v\in\mathbb{R}^{D_v\times N_X \times N_Y}\)</span>。由于视觉和音频特征具有不同的特征维度，在隐藏层中使用两个不同的仿射变换进行重新投影： <span class="math display">\[\tilde{h}_a = \mathbf{U}_a\cdot f_a+\mathbf{b}_a,\quad h_v=\mathbf{U}_v\cdot f_v+\mathbf{b}_v\]</span></p><p>其中<span class="math inline">\(\tilde{h}_a\in\mathbb{R}^{D_h},h_v\in\mathbb{R}^{D_v\times N_X \times N_Y}\)</span>，<span class="math inline">\(\mathbf{U}_a,\mathbf{b}_a,\mathbf{U}_v,\mathbf{b}_v\)</span>是对应的学习参数。此外，对音频特征应用空间平铺来匹配视觉特征的空间维度，得到<span class="math inline">\(h_a\in\mathbb{R}^{D_v\times N_X \times N_Y}\)</span>。</p><p>为了学习音频特征<span class="math inline">\(h_a\)</span>和视觉特征<span class="math inline">\(h_v\)</span>之间的对应关系，文章研究了三种方法。第一种方法不需要学习参数，直接计算两个向量之间的余弦相似度，从而得到一个定位图<span class="math inline">\(L_1\in\mathbb{R}^{N_X\times N_Y}\)</span>。第二种方法，对向量<span class="math inline">\(h_a,h_v\)</span>在像素点<span class="math inline">\((x,y)\)</span>处的内积进行加权，从而获得一个或多个定位图<span class="math inline">\(L_2^j\in\mathbb{R}^{N_X\times N_Y}, \quad j=1,\dots,N_{out}\)</span>： <span class="math display">\[L_2^j(x,y)\sum_{k=1}^{D_h}s^{j,k}\cdot h^k_v(x,y)\cdot h^k_a(x,y)+\beta^j\]</span></p><p>其中<span class="math inline">\(s^{j,k},\beta^j\)</span>是学习参数。第三种方法，也是最后使用的方法，即对输入的多模态数据应用双线性插值，也可以得到一个或多个定位图<span class="math inline">\(L_3^j\in\mathbb{R}^{N_X\times N_Y}, \quad j=1,\dots,N_{out}\)</span>： <span class="math display">\[\begin{aligned}L_3^j(x,y)=h_v(x,y)^T\cdot\mathbf{M}^j\cdot h_a(x,y)+\mu^j \\= \sum_{l=1}^{D_h}\sum_{k=1}^{D_h}M^{j,l,k}\cdot h_v^l(x,y)\cdot h_a^k(x,y)+\mu^j\end{aligned}\]</span> 其中<span class="math inline">\(M^{j,l,k},\mu^j\)</span>是学习参数。先前提到的两种方法<span class="math inline">\((L_1,L_2)\)</span>是双线性插值方法<span class="math inline">\(L_3\)</span>的特殊情况，使得输入之间有更丰富的相互作用。当矩阵<span class="math inline">\(\mathbf{M}^j\)</span>是对角矩阵，<span class="math inline">\(s^{j,k}\)</span>是对角元时，就是加权内积<span class="math inline">\((L_2)\)</span>的情况。当矩阵<span class="math inline">\(\mathbf{M}\)</span>是单位矩阵时，结果非常接近余弦相似度的版本（经过归一化因子）。 ### Audiovisual Saliency Estimation</p><p>通过声源定位图计算得到了音频显著图。但是，在一段视频中有很多方面可以吸引人的注意力，但是与音频并不相关。因此，为了构建一个多模态显著性预测网络，还需要包括由时空视觉网络建模的纯视觉信息。这也是这篇文章的一个重要贡献：提出了不同的音视融合方法。</p><p>最简单的融合方案就是学习视觉映射<span class="math inline">\(S_v\)</span>和音频相关映射<span class="math inline">\(S^a\)</span>（通过对多级级联的视觉显著性特征<span class="math inline">\(V^j=(S^1|\dots|S^m|\dots|S^M\)</span>和定位图<span class="math inline">\(L^j\)</span>分别应用全卷积层得到）的一个线性加权和：<span class="math inline">\(S_1^{av}=w_v\cdot\sigma(S^w)+w_a\cdot\sigma(S^a)\)</span>，其中<span class="math inline">\(\sigma(\cdot)\)</span>是sigmoid激活函数。</p><p>此外，受之前基于信号处理的视听显著性方法的启发，研究了一种基于注意力的方案，由音频流调制视频流：<span class="math inline">\(S_2^{av}=\sigma(S^v)\odot(1+\sigma(S^a))\)</span>。在有多个定位图的情况下，可以将级联的视觉显著性特征<span class="math inline">\(V^j\)</span>和定位图<span class="math inline">\(L^j\)</span>逐一相乘，然后应用全卷积层来获取单个显著图：<span class="math inline">\(\tilde{S}_2^{av,j}=\sigma(V^j)\odot(1+\sigma(L_j))\)</span>。</p><p>然而，如Fig2所示，文中最主要最通用的、使得视觉和音频特征映射之间有更多自由交互的方法是将多模态特征连接，然后由卷积层进行融合，得到一个显著图：<span class="math inline">\(S_3^{av}=\mathbf{W}_{cat}*(V|L)+\beta_{av}\)</span>。</p><p>最后，融合方案是之前所有方法的加权学习和：<span class="math inline">\(S^{av}_{fus}=\tilde{w}_v\cdot\sigma(S^v)+\tilde{w}_a\cdot\sigma(S^a)+w_{av}\cdot\sigma(S^3)\)</span>。</p><h3 id="saliency-losses">Saliency Losses</h3><p>为了训练与视频流相关的参数<span class="math inline">\(\mathbf{W}_v\)</span>，构建了一个损失，将显著图<span class="math inline">\(S^v\)</span>和激活<span class="math inline">\(A_m\)</span>与由眼动数据得到的ground truth <span class="math inline">\(Y_{sal}\)</span>相比较： <span class="math display">\[\begin{aligned}\mathcal{L}_v(\mathbf{W}_v)=\mathcal{D}(\mathbf{W}_v|\sigma(S^v),Y_{sal}) + \\\sum_{m=1}^4\mathcal{D}(\mathbf{W}_{AM}^m|\sigma(A^m),Y_{sal}),\end{aligned}\]</span> 其中，<span class="math inline">\(\sigma(\cdot)\)</span>是sigmoid非线性函数，<span class="math inline">\(\mathcal{D}(\cdot)\)</span>是预测与真实图像之间的损失函数。当训练音视网络的参数<span class="math inline">\(\mathbf{W}_{av}\)</span>时，使用训练过的纯视觉网络作为起点，不使用DSAM模块的跳过连接： <span class="math display">\[\mathcal{L}_{av}(\mathbf{W}_{av})=\mathcal{D}(\mathbf{W}_{av}|\sigma(S^{av}),Y_{sal}).\]</span> 为了比较预测的显著图<span class="math inline">\(P\in[0,1]^{N_X\times N_Y}\)</span>和眼动数据应用了不同的评估指标。对于真实图像，既可以使用大小为<span class="math inline">\({N_X\times N_Y}\)</span>的图像平面上的固定位置<span class="math inline">\(Y_{fix}\in{0,1}^{N_X\times N_Y}\)</span>，也可以使用由二元固定图和高斯核卷积得到的稀疏显著图<span class="math inline">\(Y_{den}\in[0,1]^{N_X\times N_Y}\)</span>。所以，对于<span class="math inline">\(\mathcal{D}(\cdot)\)</span>，使用了评估显著性预测不同方面相关联的三种损失函数。第一种是预测图<span class="math inline">\(P\)</span>和稀疏图<span class="math inline">\(Y_{den}\)</span>之间的交叉熵损失： <span class="math display">\[\begin{aligned}\mathcal{D}_{CE}(\mathbf{W}|P,Y_{den})=-\sum_{x,y}Y_{den}(x,y)\odot\log(P(x,y;\mathbf{W})) \\+(1-Y_{den}(x,y))\odot(1-\log(P(x,y;\mathbf{W}))).\end{aligned}\]</span> 第二种损失函数基于线性相关系数，广泛应用于显著性预测评估，用来衡量预测图<span class="math inline">\(P\)</span>和稀疏图<span class="math inline">\(Y_{den}\)</span>之间的线性关系： <span class="math display">\[\mathcal{D}_{CC}(\mathbf{W}|P,Y_{den})=-\frac{\operatorname{cov}(P(x,y;\mathbf{W}),Y_{den}(x,y))}{\rho(P(x,y;\mathbf{W})\cdot\rho(Y_{den}(x,y))}\]</span> 其中<span class="math inline">\(\operatorname{cov},\rho\)</span>分别代表协方差和标准差。最后一种损失函数是由Normalized Scanpath Saliency（NSS）得来的，计算预测图<span class="math inline">\(\tilde{P}(x,y;\mathbf{W})=\frac{P(x,y;\mathbf{W})-\mu(P(x,y;\mathbf{W}))}{\rho(P(x,y;\mathbf{W}))}\)</span>，经过零均值归一化和单位标准化后，在人注视的位置(<span class="math inline">\(Y_{fix}(x,y)=1\)</span>)： <span class="math display">\[\mathcal{D}_{NSS}(\mathbf{W}|\tilde{P},Y_{fix})=-\frac{1}{N_f}\sum_{x,y}\tilde{P}(x,y;\mathbf{W})\odot Y_{fix}(x,y).\]</span> 其中<span class="math inline">\(N_f=\sum_{x,y}Y_{fix}(x,y)\)</span>是所有注视点的总数。对于第i个输入样例的最终损失函数为分别使用对应损失<span class="math inline">\(\mathcal{D}^i_{CE},\mathcal{D}^i_{CC},\mathcal{D}^i_{NSS}\)</span>的损失函数<span class="math inline">\(\mathcal{L}^i_{CE},\mathcal{L}^i_{CC},\mathcal{L}^i_{NSS}\)</span>的加权和： <span class="math display">\[\mathcal{L}^i_{sal}(\mathbf{W})=w_1\mathcal{L}^i_{CE}+w_2\mathcal{L}^i_{CC}+w_3\mathcal{L}^i_{NSS}\]</span> 其中<span class="math inline">\(w_1,w_2,w_3\)</span>是每种损失的权重。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>Saliency Prediction</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>动手学深度学习</title>
    <link href="/2022/11/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    <url>/2022/11/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="动手学深度学习">动手学深度学习</h1><p>GitHub地址：<img src="https://github.com/d2l-ai/d2l-zh" alt="https://github.com/d2l-ai/d2l-zh" /></p><h2 id="线性回归网络">线性回归网络</h2><h3 id="线性回归">线性回归</h3><h4 id="线性模型">线性模型</h4><p><span class="math display">\[\hat{\mathbf{y}}=\mathbf{Xw} + b\]</span></p><h4 id="损失函数">损失函数</h4><p><span class="math display">\[l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.\]</span></p><p><span class="math display">\[L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.\]</span></p><p><span class="math display">\[\mathbf{w}^*, b^* = \operatorname*{argmin}_{\mathbf{w}, b}\  L(\mathbf{w}, b).\]</span></p><h4 id="解析解">解析解</h4><p>线性回归的解可以用一个共识简单地表达出来，这类解叫做解析解。将偏置<span class="math inline">\(b\)</span>合并到参数<span class="math inline">\(\mathbf{w}\)</span>中，合并方法是所有参数的矩阵中附加一列。预测问题是最小化<span class="math inline">\(\|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2\)</span>。这在损失平面上只有一个临界点，这个临界点对应于整个区域的损失极小点。将损失关于<span class="math inline">\(\mathbf{w}\)</span>的导数设为0，得到解析解： <span class="math display">\[\mathbf{w}^* = (\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf{y}.\]</span> 但不是所有问题都存在解析解。解析解可以进行很好的数学分析，但是对问题的限制很严格，无法广泛应用于深度学习。</p><h4 id="随机梯度下降">随机梯度下降</h4><p>随机梯度下降通过不断地在损失函数递减的方向上更新参数来降低误差。</p><p>梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值）关于模型参数的导数（在这里可以称为梯度）。但是由于整个数据集可能会很大，遍历一遍的速度很慢，所以可以随机抽取一小批样本，这样的变体称为小批量随机梯度下降。</p><p>对于每次迭代，先随机抽样一个小批量<span class="math inline">\(\mathcal{B}\)</span>，计算小批量的平均损失关于模型参数的导数（或梯度）。最后，将梯度乘学习率<span class="math inline">\(\eta\)</span>,并从当前参数中减掉。</p><p>用数学公式表示为（<span class="math inline">\(\partial\)</span>表示偏导数）： <span class="math display">\[(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).\]</span> <span class="math display">\[\begin{split}\begin{aligned} \mathbf{w} &amp;\leftarrow \mathbf{w} -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right),\\ b &amp;\leftarrow b -  \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_b l^{(i)}(\mathbf{w}, b)  = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right). \end{aligned}\end{split}\]</span></p><h4 id="正态分布与平方损失">正态分布与平方损失</h4><p>若随机变量<span class="math inline">\(x\)</span>具有均值<span class="math inline">\(\mu\)</span>和方差<span class="math inline">\(\sigma^2\)</span>（标准差<span class="math inline">\(\sigma\)</span>），其正态分布概率密度函数如下： <span class="math display">\[p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (x - \mu)^2\right).\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">normal</span>(<span class="hljs-params">x, mu, sigma</span>):<br>    p = <span class="hljs-number">1</span> / math.sqrt(<span class="hljs-number">2</span> * math.pi * sigma**<span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">return</span> p * np.exp(-<span class="hljs-number">0.5</span> / sigma**<span class="hljs-number">2</span> * (x - mu)**<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><p>均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是：假设观测中包含噪声，其中噪声服从正态分布。噪声正态分布如下式： <span class="math display">\[y = \mathbf{w}^\top \mathbf{x} + b + \epsilon,\]</span> 其中，<span class="math inline">\(\epsilon \sim \mathcal{N}(0, \sigma^2)\)</span>。</p><p>因此，我们现在可以写出通过给定<span class="math inline">\(\mathbf{x}\)</span>的观测到特定<span class="math inline">\(y\)</span>的似然（likelihood）: <span class="math display">\[P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right).\]</span> 现在，根据极大似然估计法，参数<span class="math inline">\(\mathbf{w}\)</span>和<span class="math inline">\(b\)</span>的最优值是使整个数据集的似然最大的值： <span class="math display">\[P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)}|\mathbf{x}^{(i)}).\]</span> 使用对数来简化指数函数的乘积最大化问题，变为最小化负对数似然<span class="math inline">\(-\log P(\mathbf y \mid \mathbf X)\)</span>： <span class="math display">\[-\log P(\mathbf y \mid \mathbf X) = \sum_{i=1}^n \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b\right)^2.\]</span></p><h3 id="softmax回归">softmax回归</h3><h4 id="softmax运算">softmax运算</h4><p>不能将未规范化的预测<span class="math inline">\(o\)</span>直接输出，因为没有限制这些数字的总和为1，且可能出现负值。运用softmax函数进行校准： <span class="math display">\[\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{其中}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}\]</span> 这里，对于所有的<span class="math inline">\(j\)</span>总有<span class="math inline">\(0 \leq \hat{y}_j \leq 1\)</span>。输出最有可能的类别： <span class="math display">\[\operatorname*{argmax}_j \hat y_j = \operatorname*{argmax}_j o_j.\]</span></p><h4 id="小批量样本的矢量化">小批量样本的矢量化</h4><p>读入一个批量的样本<span class="math inline">\(\mathbf{X}\)</span>，特征维度（输入数量）为<span class="math inline">\(d\)</span>，批量大小为<span class="math inline">\(n\)</span>。输出时有<span class="math inline">\(q\)</span>个类别。那么小批量样本的特征为<span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times d}\)</span>，权重为<span class="math inline">\(\mathbf{W} \in \mathbb{R}^{d \times q}\)</span>，偏置为<span class="math inline">\(\mathbf{b} \in \mathbb{R}^{1\times q}\)</span>。softmax回归的矢量计算表达式为： <span class="math display">\[\begin{split}\begin{aligned} \mathbf{O} &amp;= \mathbf{X} \mathbf{W} + \mathbf{b}, \\ \hat{\mathbf{Y}} &amp; = \mathrm{softmax}(\mathbf{O}). \end{aligned}\end{split}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>PyTorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[论文阅读]AdaMixer: A Fast-Converging Query-Based Object Detector</title>
    <link href="/2022/11/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-AdaMixer-A-Fast-Converging-Query-Based-Object-Detector/"/>
    <url>/2022/11/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-AdaMixer-A-Fast-Converging-Query-Based-Object-Detector/</url>
    
    <content type="html"><![CDATA[<p>TransformerEncoder、MultiScale Deformable TransformerEncoder、FPN增加了计算成本，训练需要大量的时间和数据。</p><p>提高跨图像解码查询的适应性</p><h5 id="object-query-decoder-revisited">Object Query Decoder Revisited</h5><figure><img src="https://k0145vin.oss-cn-hangzhou.aliyuncs.com/Image202211031125595.png" alt="Comparisons" /><figcaption aria-hidden="true">Comparisons</figcaption></figure><h5 id="our-object-query-definition">Our Object Query Definition</h5><p>根据object query的定义，还是将一个query解耦为两个向量：content vector <span class="math inline">\(\mathrm{q}\)</span>和positional vetor <span class="math inline">\((x,y,z,r)\)</span>。content vector <span class="math inline">\(\mathbf{q}\)</span>是<span class="math inline">\(\mathbb{R}^{d_q}\)</span>中的一个向量，<span class="math inline">\(d_q\)</span>是通道维数。向量<span class="math inline">\((x,y,z,r)\)</span>中<span class="math inline">\(x,y\)</span>是bounding box的中心坐标，<span class="math inline">\(z\)</span>是框大小的对数，<span class="math inline">\(r\)</span>是长宽比的对数。<span class="math inline">\(x,y,z\)</span>还可以直接表示3D特征空间中query的坐标。</p><p><strong>Decoding the bounding box from a query</strong></p><p>可以从位置编码中解码边界框信息。中心点<span class="math inline">\((x_B,y_B)\)</span>，宽和高<span class="math inline">\(w_B,h_B\)</span>解码： <span class="math display">\[x_B=s_{base}\cdot x,y_B=s_{base}\cdot y\]</span></p><p><span class="math display">\[w_B=s_{base}\cdot 2^{z-r},h_B=s_{base}\cdot 2^{z+r}\]</span></p><p><span class="math inline">\(s_{base}\)</span>是下采样步长偏移量，根据实验一般设置为<span class="math inline">\(s_{base}=4\)</span>。</p><h5 id="adaptive-location-sampling">Adaptive Location Sampling</h5><p>decoder应该自适应地针对query决定采样哪个特征。也就是说，decoder应该同时考虑位置向量<span class="math inline">\((x,y,z,r)\)</span>和内容向量<span class="math inline">\(\mathbf{q}\)</span>对采样位置进行采样位置进行解码。此外，decoder不仅要在<span class="math inline">\((x,y)\)</span>空间上是自适应的，在潜在目标的尺度上也要是自适应的。具体来说，通过将多尺度特征看作一个三维特征空间，自适应地从中采样。</p><p><strong>Multi- scale features as the 3D feature space</strong></p><p>给定一个索引为<span class="math inline">\(j\)</span>，下采样步长为<span class="math inline">\(s_j^{feat}\)</span>的特征图，先通过线性映射到相同的通道数<span class="math inline">\(d_{feat}\)</span>，然后计算z轴坐标： <span class="math display">\[z_j^{feat}=\log_2(s_j^{feat}/s_{base})\]</span> 然后将不同步长的特征图的高和宽重新缩放到相同的<span class="math inline">\(H/s_{base},W/s_{base}\)</span>，其中<span class="math inline">\(H,W\)</span>是输入图像的高和宽，并将它们放在3D空间中的x轴和y轴上对齐。</p><figure><img src="https://k0145vin.oss-cn-hangzhou.aliyuncs.com/Image202211041039003.png" alt="3D feature sampling process" /><figcaption aria-hidden="true">3D feature sampling process</figcaption></figure><p><strong>Adaptive 3D feature sampling process</strong></p><p>一个query首先通过一个线性层生成针对点的偏移向量集合<span class="math inline">\(P_{in},\{(\Delta x_i,\Delta y_i, \Delta z_i)\}_{P_{in}}\)</span>： <span class="math display">\[\{(\Delta x_i,\Delta y_i, \Delta z_i)\}_{P_{in}}={\rm Linear}(\mathbf{q})\]</span> 然后，根据query <span class="math inline">\(i\)</span>的的位置向量将这些偏移量转化为采样位置： <span class="math display">\[\left\{\begin{matrix} \tilde{x}_i=x+\Delta x_i\cdot 2^{z-r},\\ \tilde{y}_i=y+\Delta y_i\cdot 2^{z+r}, \\\tilde{z}_i=z+\Delta z_i,\end{matrix}\right.\]</span> 值得注意的是，从query中解码用来描述bounding box的区域<span class="math inline">\(\{\Delta x_i,\Delta y_i\in[-0.5,0.5]\}\)</span>。偏移量并不限于此范围内，这就意味着query可以在bounding box外进行采样。得到集合后，采样器即可在3D空间内对给定的点进行采样。在现有的实现中，3D空间内的插值是组合实现的：首先在<span class="math inline">\((x,y)\)</span>平面内通过双线性插值对给定点进行采样，然后在给定采样<span class="math inline">\(\tilde{z}\)</span>的情况下通过高斯加权对z轴进行采样，其中特征图<span class="math inline">\(j\)</span>的权重为： <span class="math display">\[\tilde{w}_j=\frac{\exp(-(\tilde{z}-z_j^{feat})^2/\tau_z)}{\sum_j\exp(-(\tilde{z}-z_j^{feat})^2\tau_z)}\]</span> 其中<span class="math inline">\(\tau_z\)</span>是z轴上插值的软化系数，在本文中取<span class="math inline">\(\tau_z=2\)</span>。特征图的通道数为<span class="math inline">\(d_{feat}\)</span>，采样特征矩阵<span class="math inline">\(\mathbf{x}\)</span>的形状为<span class="math inline">\(\mathbb{R}^{p_{in}\times d_{feat}}\)</span>。通过利用显式、自适应的一致的位置信息和与query对应的尺度进行采样，自适应3D特征采样处理简化了decoder 的学习过程。</p><p><strong>Group sampling</strong></p><p>为了尽可能采样多点的特征，引入了群体采样机制，类似于注意力机制中的多头注意力和群体卷积。群体采样首先将3D特征空间的通道数<span class="math inline">\(d_{feat}\)</span>分成<span class="math inline">\(g\)</span>组，每组为<span class="math inline">\(d_{feat}/g\)</span>，每组分别进行3D采样。采用分组采样机制，对于每个query，decoder可以生成<span class="math inline">\(g\cdot P_{in}\)</span>组偏移向量，来丰富采样点的多样性，利用这些点更丰富的空间结构特征。采样特征矩阵<span class="math inline">\(\mathbf{x}\)</span>的形状就变成了<span class="math inline">\(\mathbb{R}^{g\times P_{in}\times (d_{feat}/g)}\)</span>。</p><h5 id="adaptive-content-decoding">Adaptive Content Decoding</h5><p>为了捕获<span class="math inline">\(\mathbf{x}\)</span>空间和通道维数的相关性，提出了分别对每个维数中的内容的有效解码。设计了一个MLP-mixer的简化、自适应变体，进行自适应混合，动态混合权重类似于卷积中的dynamic filters。</p><figure><img src="https://k0145vin.oss-cn-hangzhou.aliyuncs.com/Image202211041455275.png" alt="Adaptive mixing procedure" /><figcaption aria-hidden="true">Adaptive mixing procedure</figcaption></figure><p>如上图所示，在query的参与下，依次进行自适应通道混合和自适应空间混合，以利用自适应通道语义信息和空间结构。</p><p><strong>Adaptive channel mixing</strong></p><p>给定一个query的采样特征矩阵<span class="math inline">\(\mathbf{x}\in\mathbb{R}^{P_{in}\times C}\)</span>，其中<span class="math inline">\(C=d_{feat/g}\)</span>，自适应通道混合（ACM）会使用基于<span class="math inline">\(\mathbf{q}\)</span>的动态权重在通道维数对<span class="math inline">\(\mathbf{x}\)</span>进行转换以自适应地增强通道语义： <span class="math display">\[M_c={\rm Linear}(\mathbf{q})\in\mathbb{R}^{C\times C}\]</span></p><p><span class="math display">\[{\rm ACM}(\mathbf{x})={\rm ReLU(LayerNorm}(\mathbf{x}M_c)),\]</span></p><p>其中<span class="math inline">\({\rm ACM}(\mathbf{x})\in\mathbb{R}^{P_{in}\times C}\)</span>是通道混合特征的输出，线性层对于每一组都是独立的。然后对混合输出的所有维度都进行层正则化。在这一步中，动态权重在3D空间中的不同采样点之间是共享的，类似于Sparse R-CNN中RoI特征的<span class="math inline">\(1\times1\)</span>的自适应卷积。</p><p><strong>Adaptive spatial mixing</strong></p><p>为了使query对采样特征的空间结构自适应，引入了自适应空间混合（ASM）。如上图所示，先对通道混合特征矩阵进行转置，然后对其空间维度应用动态核： <span class="math display">\[M_s={\rm Linear}(\mathbf{q})\in\mathbb{R}^{P_{int}\times P_{out}}\]</span></p><p><span class="math display">\[{\rm ASM}(\mathbf{x})={\rm ReLU(LayerNorm)}(\mathbf{x}^T M_s),\]</span></p><p>其中<span class="math inline">\({\rm ASM}(\mathbf{x})\in\mathbb{R}^{C\times P_{out}}\)</span>是空间混合输出，<span class="math inline">\(P_{out}\)</span>是空间混合输出数量。动态权重在不同通道间是共享的。由于采样点可能来自不同的特征尺度，ASM自然需要涉及多尺度交互建模，这对于实现高性能目标检测是非常必要的。</p><h5 id="overall-adamixer-detector">Overall AdaMixer Detector</h5><figure><img src="https://k0145vin.oss-cn-hangzhou.aliyuncs.com/Image202211081547831.png" alt="decoder structure of the AdaMixer" /><figcaption aria-hidden="true">decoder structure of the AdaMixer</figcaption></figure><p>query的位置向量在阶段结束后由另一个FFN来做更新： <span class="math display">\[x^\prime = x+\Delta x\cdot 2^z,y^\prime=t+\Delta y\cdot2^z,\]</span></p><p><span class="math display">\[z^\prime=z+\Delta z,r^\prime=r+\Delta r,\]</span></p><p>其中<span class="math inline">\((\Delta x,\Delta y, \Delta z,\Delta r)\)</span>是由小的FFN块产生的。</p><p><strong>Position-aware multi-head self-attentions</strong></p><p>由于对query进行了解耦，分离出了内容和位置向量，内容向量间原始的多头自注意力机制不知道一个query与另外一个query之间的几何关系，这被证明有利于抑制冗余检测。为了实现这一点，我们将位置信息嵌入到自注意力中。正弦形式的内容向量的位置信息和<span class="math inline">\((x,y,z,r)\)</span>组件嵌入占用了四分之一的通道。我们还将前景交集（IoF）作为query间注意力权重的偏置嵌入到query中，以显式地包含query间被包含的关系。对于每个注意力头： <span class="math display">\[{\rm Attn}(Q,K,V)={\rm Softmax}(QK^T/\sqrt{d_q}+\alpha B)V,\]</span> 其中<span class="math inline">\(B_{ij}=\log(|box_i\cap box_j|/|box_i|+\epsilon)\)</span>,<span class="math inline">\(\epsilon=10^{-7}\)</span>,<span class="math inline">\(Q,K,V\in\mathbb{R}^{N\times d_q}\)</span>代表query、key和value矩阵，<span class="math inline">\(\alpha\)</span>是每个头的可学习参数。<span class="math inline">\(B_{ij}=0\)</span>代表box <span class="math inline">\(i\)</span>完全包含在box <span class="math inline">\(j\)</span>中，<span class="math inline">\(B_{ij}=\log\epsilon\ll0\)</span>代表box <span class="math inline">\(i\)</span>和<span class="math inline">\(j\)</span>之间没有重叠。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>Object Detection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[论文阅读]Accelerating DETR Convergence via Semantic-Aligned Matching</title>
    <link href="/2022/10/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Accelerating-DETR-Convergence-via-Semantic-Aligned-Matching/"/>
    <url>/2022/10/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Accelerating-DETR-Convergence-via-Semantic-Aligned-Matching/</url>
    
    <content type="html"><![CDATA[<p>收敛缓慢的原因：初始状态下，每个object query要跟所有的空间位置进行匹配，需要相当长的训练周期来学习与目标相关的区域。SMCA-DETR、Conditional DETR、Deformable DETR中都有提及。</p><p><img src="https://k0145vin.oss-cn-hangzhou.aliyuncs.com/Image202210292148854.png" /></p><p>造成object query无法正确聚焦于特定区域的原因是Cross-Attention之间的多个模块（Self-Attention和FFN）对object query进行了多次映射，使得object query与图像特征F的语义未对齐，也就是说，object query和图像特征F被映射到了不同的嵌入空间（Embedding Space）内。</p><p>Deformable DETR用可形变注意力机制代替原来的全局密集注意力，只关注小部分的特征</p><p>Conditional DETR、SMCA-DETR将交叉注意力模块改为空间条件化的。</p><p>Motivation: Siamese-based architecture 孪生结构</p><figure><img src="https://k0145vin.oss-cn-hangzhou.aliyuncs.com/Image202210292147420.jpeg" alt="decoder_layer" /><figcaption aria-hidden="true">decoder_layer</figcaption></figure><p>通过两个相同的子网络将匹配双方的语义对齐，以投影到相同的embedding space，便于后续匹配。为object query提供了一个强大的先验，使其只关注于语义相似的区域上。</p><p>物体的关键点和端点在目标识别和定位中非常重要，显式地搜索多个显著点，并用于语义对齐匹配</p><p>交叉注意力模块-&gt;匹配与蒸馏 <span class="math display">\[\rm\mathbf{Q}^{\prime}=\underbrace{\overbrace{Softmax(\frac{(\mathbf{QW}_q)(\mathbf{FW}_k)^T}{\sqrt{d}}}^{匹配相关区域}(\mathbf{FW}_v)}_{从匹配区域中提取特征}\]</span> 给定编码图像特征<span class="math inline">\(\mathbf{F}\)</span>和object queries的参考框<span class="math inline">\(\mathbf{R}_{box}\)</span>，<span class="math inline">\(HW\times d\Rightarrow H\times W \times d\)</span>，RoIAlign： <span class="math display">\[\rm\mathbf{F}_R=RoIAlign(\mathbf{F},\mathbf{R}_{box})\]</span> <span class="math inline">\(\rm\mathbf{F}_R\in \mathbb{R}^{N\times 7\times 7\times d}\)</span> <span class="math display">\[\rm\mathbf{Q}^{new},\mathbf{Q}_{pos}^{new}=Resample(\mathbf{F}_{R},\mathbf{R}_{box},\mathbf{Q})\]</span> <span class="math inline">\(\rm\mathbf{Q}^{new},\mathbf{Q}_{pos}^{new}\)</span>是通过<span class="math inline">\(\rm\mathbf{F}_R\)</span>重采样得到的，不涉及任何投影，因而object query embedding<span class="math inline">\(\rm\mathbf{Q}^{new}\)</span>跟<span class="math inline">\(\mathbf{F}\)</span>共享相同的嵌入空间，为object queries提供先验关注语义相似的区域。</p><p>对于检测任务来说，物体的显著点是识别和定位的关键。因此选择显著点的特征作为Semantics Aligner的输出。</p><p>假设注意力头的数量为<span class="math inline">\(M\)</span>，通常设置为8。通过RoIAlign得到特征<span class="math inline">\(\rm\mathbf{F}_R\)</span>后，通过ConvNet和多层感知器每个区域预测<span class="math inline">\(M\)</span>个显著点坐标，<span class="math inline">\(\rm{\mathbf{R}_{SP}}\in\mathbb{R}^{N\times M\times 2}\)</span>. <span class="math display">\[\rm\mathbf{R}_{SP}=MLP(ConvNet(\mathbf{F}_R))\]</span> 显著点坐标的预测是限制在参考框内的，后面会进行解释。显著点的特征随后通过双线性插值从<span class="math inline">\(\rm\mathbf{F}_{R}\)</span>中采样得到。<span class="math inline">\(M\)</span>个采样特征向量连接起来作为新的object query embedding，使得每个注意力头关注一个显著点的特征。 <span class="math display">\[{\rm\mathbf{Q}^{new\prime}=Concat({\mathbf{F}_{R}}}[\dots,x,y,\dots])\,{\rm for}\, x,y\in\rm{\mathbf{R}_{SP}}\]</span></p><p><span class="math display">\[\rm\mathbf{Q}_{pos}^{new\prime}=Concat(Sinusoidal(\mathbf{R}_{box},\mathbf{R}_{SP}))\]</span></p><p>Semantics Aligner高效地产生了与编码图像特征语义对齐的object queries，但是同时也产生了问题：之前的query embedding <span class="math inline">\(\mathbf{Q}\)</span>包含的对检测有价值的信息并没有应用到交叉注意力模块中。</p><p>又利用先前的query embedding作为输入通过线性投影和sigmoid函数产生了重加权系数。通过与重加权系数相乘，新产生的query embedding和对应的position embedding都被重加权来突出重要特征，有效地利用先前的有价值的信息 <span class="math display">\[\rm\mathbf{Q}^{new}=\mathbf{Q}^{new\prime}\otimes \sigma(\mathbf{QW}_{RW1})\]</span></p><p><span class="math display">\[\rm\mathbf{Q}_{pos}^{new}=\mathbf{Q}_{pos}^{new\prime}\otimes \sigma(\mathbf{QW}_{RW2})\]</span></p><p><span class="math inline">\(\rm\mathbf{W}_{RW1},\mathbf{W}_{RW2}\)</span>是线性投影，<span class="math inline">\(\sigma\)</span>是sigmoid函数，<span class="math inline">\(\otimes\)</span>是逐元素相乘</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>Object Detection</tag>
      
      <tag>DETR-like</tag>
      
      <tag>transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[论文阅读] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</title>
    <link href="/2022/10/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Transformers-are-RNNs-Fast-Autoregressive-Transformers-with-Linear-Attention/"/>
    <url>/2022/10/29/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Transformers-are-RNNs-Fast-Autoregressive-Transformers-with-Linear-Attention/</url>
    
    <content type="html"><![CDATA[<p>之前的工作证明了多头自注意力只要有足够的注意力头数就可以表示任意的卷积层。但是，本文反向表明，用自回归目标训练的自注意力层可以被看作是一个RNN，可以显著加快自回归transformer模型的推理时间。</p><h5 id="transformer">Transformer</h5><p><span class="math inline">\(x\in\mathbb{R}^{N\times F}\)</span>,<span class="math inline">\(N\)</span>个<span class="math inline">\(F\)</span>维的特征向量。Transformer即一个函数<span class="math inline">\(T:\mathbb{R}^{N\times F}\rightarrow\mathbb{R}^{N\times F}\)</span>，由<span class="math inline">\(L\)</span>个transformer层<span class="math inline">\(T_{1}(\cdot),\dots,T_{L}(\cdot)\)</span>组成: <span class="math display">\[T_{l}(x)=f_{l}(A_{l}(x)+x).\]</span> <span class="math inline">\(A_l(\cdot)\)</span>代表自注意力函数。输入序列<span class="math inline">\(x\)</span>由三个矩阵<span class="math inline">\(W_Q\in\mathbb{R}^{F\times D}，W_K\in\mathbb{R}^{F\times D},W_v\in\mathbb{R}^{F\times M}\)</span>映射到<span class="math inline">\(Q,K,V\)</span>，<span class="math inline">\(A_l(x)=V^\prime\)</span> <span class="math display">\[Q=xW_Q,\\K=xW_K,\\V=xW_V,\\A_l(x)=V^\prime={\rm softmax}(\frac{QK^T}{\sqrt{D}})V.\]</span> softmax函数按行应用于<span class="math inline">\(QK^T\)</span>,<span class="math inline">\(Q,K,V\)</span>分别表示queries、keys和values。</p><p>式2表示了一种特定形式的注意力，称为softmax注意力，其中相似性是由<span class="math inline">\(Q\)</span>和<span class="math inline">\(K\)</span>的点积的指数表示的。给定一个下标<span class="math inline">\(i\)</span>，返回一个矩阵的第<span class="math inline">\(i\)</span>行作为一个向量，对于任意相似性函数，可以写出一个广义的注意力方程： <span class="math display">\[V_i^\prime=\frac{\sum_{j=1}^N{\rm sim}(Q_i,K_j)V_j}{\sum_{j=1}^N{\rm sim}(Q_i,K_j)}\]</span> 将式3中的相似性函数<span class="math inline">\({\rm sim}(q,k)\)</span>替代为<span class="math inline">\(\exp(\frac{q^Tk}{\sqrt{D}})\)</span>，则与式2等价。</p><h5 id="linearized-attention">Linearized Attention</h5><p>式2中注意力的定义具有一般性，可以用来定义一些其他的注意力，如多项式注意力和RBF核注意力。为了使式3定义一个注意力函数，需要对<span class="math inline">\(sim(\cdot)\)</span>施加一个非负的约束。<span class="math inline">\(k(x,y):\mathbb{R}^{2\times F}\rightarrow\mathbb{R}_{+}\)</span></p><p>给定一个特征表示核函数<span class="math inline">\(\phi(x)\)</span>，则可以将式2重写为： <span class="math display">\[V_i^\prime=\frac{\sum_{j=1}^N\phi(Q_i)^T\phi(K_j)V_j}{\sum_{j=1}^N\phi(Q_i)^T\phi(K_j)}\]</span> 根据矩阵乘法的结合律，进一步简化： <span class="math display">\[V_i^\prime=\frac{\phi(Q_i)^T\sum_{j=1}^N\phi(K_j)V_j^T}{\phi(Q_i)^T\sum_{j=1}^N\phi(K_j)}\]</span> 当分子写成向量形式时，式5可以更简化 <span class="math display">\[(\phi(Q)\phi(K)^T)V=\phi(Q)(\phi(K)^TV)\]</span> 特征映射<span class="math inline">\(\phi(\cdot)\)</span>逐行应用于矩阵<span class="math inline">\(Q,K\)</span></p><p>对于式2，softmax注意力的计算复杂度式<span class="math inline">\(\mathcal{O}(N^2)\)</span>的，<span class="math inline">\(N\)</span>表示序列长度。空间复杂度也是相同的，因为要保存完整的注意力矩阵来计算<span class="math inline">\(Q,K,V\)</span>的梯度。</p><p>对于式5，linear transformer的时间复杂度、空间复杂度都是<span class="math inline">\(\mathcal{O}(N)\)</span>的，因为我们可以一次计算出<span class="math inline">\(\sum_{j=1}^N\phi(K_j)V_j^T\)</span>和<span class="math inline">\(\sum_{j=1}^N\phi(K_j)\)</span>并且在每个查询中重复使用。</p><p>对于softmax注意力，乘法和加法的总复杂度为<span class="math inline">\(\mathcal{O}(N^2\max(D,M))\)</span>，<span class="math inline">\(D\)</span>是<span class="math inline">\(Q,K\)</span>的维度，<span class="math inline">\(M\)</span>是<span class="math inline">\(V\)</span>的维度。</p><p>对于线性注意力，首先计算维度为<span class="math inline">\(C\)</span>的特征图，然后计算新值的加法和乘法的复杂度为<span class="math inline">\(\mathcal{O}(NCM)\)</span>。</p><p>先前的分析中并没有考虑到核函数和特征函数的选择。与指数核对应的特征函数是无穷维的，这导致不能精确地线性化softmax注意力。而另一方面，多项式核具有精确的、有限维的特征映射，并且已被证明与指数核或RBF核同样有效。计算一个2次线性化多项式transformer的复杂度为<span class="math inline">\(\mathcal{O}(ND^2M)\)</span>。</p><p>对于小序列，使用一个特征映射得到正的相似度 <span class="math display">\[\phi(x)={\rm elu}(x)+1\]</span> 相较于<span class="math inline">\({\rm relu}(\cdot)\)</span>，<span class="math inline">\({\rm elu}(\cdot)\)</span>可以避免x为负时将梯度设置为0。这样的特征映射产生的注意力计算复杂度为<span class="math inline">\(\mathcal{O}(NDM)\)</span>。</p><h5 id="causal-masking">Causal Masking</h5><p>利用transformer框架可以通过掩盖注意力高效地训练自回归模型，使得第<span class="math inline">\(i\)</span>个位置只能受到第<span class="math inline">\(j\)</span>个位置的影响，当且仅当<span class="math inline">\(j\leq i\)</span>，即一个位置不能受到后续位置的影响。由此将式3改写为： <span class="math display">\[V_i^\prime=\frac{\sum_{j=1}^i{\rm sim}(Q_i,K_j)V_j}{\sum_{j=1}^i{\rm sim}(Q_i,K_j)}.\]</span> 又由之前的推理，将掩蔽注意力线性化如下： <span class="math display">\[V_i^\prime=\frac{\phi(Q_i)^T\sum_{j=1}^i\phi(K_j)V_j^T}{\phi(Q_i)^T\sum_{j=1}^i\phi(K_j)}.\]</span> 引入<span class="math inline">\(S_i,Z_i\)</span>如下： <span class="math display">\[S_i=\sum_{j=1}^i\phi(K_j)V_j^T,\]</span></p><p><span class="math display">\[Z_i=\sum_{j=1}^i\phi(K_j),\]</span></p><p>将式9仅一步简化： <span class="math display">\[V_i^\prime=\frac{\phi(Q_i)^TS_i}{\phi(Q_I)^TZ_i}\]</span> <span class="math inline">\(S_i,Z_i\)</span>是可以由<span class="math inline">\(S_{i-1},Z_{i-1}\)</span>连续计算得到的，因而使得带有因果掩码的linear transformer的计算复杂度与序列长度称线性关系。</p><p><strong>gradient computation</strong></p><p>在任何深度学习框架中，式12的朴素实现都需要存储所有的中间值<span class="math inline">\(S_i\)</span>以计算梯度，这使得内存的消耗量最大增加<span class="math inline">\(\max(D,M)\)</span>倍，影响了对长序列或者深度模型的适用性。为解决这一问题，导出式9中的分子的梯度作为累加和。这使得我们可以在线性时间和固定的内存空间同时计算causal linear attention的前向和后向传播。</p><p>给定分子<span class="math inline">\(\bar{V_i}\)</span>和标量损失函数对于分子<span class="math inline">\(\bar{V_i}\)</span>的梯度<span class="math inline">\(\nabla_{\bar{V_i}}\mathcal{L}\)</span>，导出<span class="math inline">\(\nabla_{\phi(Q_i)\mathcal{L}},\nabla_{\phi(K_i)}\mathcal{L},\nabla_{V_i}\mathcal{L}\)</span>如下： <span class="math display">\[\nabla_{\phi(Q_i)\mathcal{L}}=\nabla_{\bar{V_i}}\mathcal{L}{\Bigg(}\sum_{j=1}^i\phi(K_j)V_j^T{\Bigg)}^T,\]</span></p><p><span class="math display">\[\nabla_{\phi(K_i)\mathcal{L}}={\Bigg(}\sum_{j=1}^N\phi(Q_j)\Big(\nabla_{\bar{V_i}}\mathcal{L}\Big)^T{\Bigg)}V_i,\]</span></p><p><span class="math display">\[\nabla_{V_i}\mathcal{L}={\Bigg(}\sum_{j=1}^N\phi(Q_j)\Big(\nabla_{\bar{V_i}}\mathcal{L}\Big)^T{\Bigg)}^T\phi(K_i).\]</span></p><p>式9、式13-15的累加和是在线性时间内、仅需关于序列长度线性比的内存空间内计算得到的。给定一个<span class="math inline">\(C\)</span>维的特征图，算法的时间复杂度为<span class="math inline">\(\mathcal{O}(NCM)\)</span>，空间复杂度为<span class="math inline">\(\mathcal{O}(N\max(C,M))\)</span></p><h5 id="summary">Summary</h5><p>这篇文章实现了线性复杂度的transformer，后续尝试把线性的transformer加到DETR类模型里跑一下，先从original DETR开始改。Facebook有后续的工作，<a href="http://arxiv.org/abs/2209.07484">Hydra Attention</a>，但是还没有开源，先挖个坑后面再看。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[论文阅读] Attention is All you Need</title>
    <link href="/2022/10/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Attention-is-All-you-Need/"/>
    <url>/2022/10/28/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Attention-is-All-you-Need/</url>
    
    <content type="html"><![CDATA[<p>encoder将符号表示的输入序列<span class="math inline">\((x_1,\dots,x_n)\)</span>映射到连续表示的序列<span class="math inline">\((z_1,\dots,z_n)\)</span>。给定<span class="math inline">\(z\)</span>，decoder再逐元素生成输出的符号序列<span class="math inline">\((y_1,\dots,y_n)\)</span>。</p><h5 id="encoder">Encoder</h5><p>encoder是由6个相同的层堆叠而成的，每层又包含2个字层，第一层是多头注意力机制，第二层是简单的全连接前馈网络。两个子层之间使用残差连接，然后进行归一化，即每个子层的输出为<span class="math inline">\({\rm LayerNorm}(x + {\rm Sublayer}(x))\)</span>。为了便于实现残差连接，所有输出的维数均为<span class="math inline">\(d_{model}=512\)</span></p><h5 id="decoder">Decoder</h5><p>decoder也是由6个相同的层堆叠而成的，除了先前提到的两个子层外，decoder还添加了第三个子层，对encoder的输出进行多头注意力。跟encoder相似，decoder在每个子层间也添加了残差连接，然后进行归一化。此外，decoder中的自注意力添加了mask机制，确保对于位置<span class="math inline">\(i\)</span>的预测是依赖小于<span class="math inline">\(i\)</span>的已知位置输出得到的。</p><figure><img src="https://k0145vin.oss-cn-hangzhou.aliyuncs.com/Image202210292117897.png" alt="transformer" /><figcaption aria-hidden="true">transformer</figcaption></figure><h5 id="attention">Attention</h5><p>attention函数可以理解为将一个查询和一组键值映射到一个输出上。</p><figure><img src="https://k0145vin.oss-cn-hangzhou.aliyuncs.com/Image202210292120164.png" alt="attention" /><figcaption aria-hidden="true">attention</figcaption></figure><h5 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h5><p><span class="math display">\[{\rm Attention}(Q,K,V)={\rm softmax}(\frac{QK^T}{\sqrt{d_k}})V\]</span></p><h5 id="multi-head-attention">Multi-Head Attention</h5><p>与单一的注意力函数统一投影到<span class="math inline">\(d_{model}\)</span>维不同，将查询、键值分别投影到<span class="math inline">\(d_k,d_k,d_v\)</span>维有收益。在查询、键值的这些映射版本上并行执行注意力机制，生成<span class="math inline">\(d_v\)</span>维的输出，最终连接起来再次映射，得到最终输出。</p><p><span class="math inline">\({\rm MultiHead}(Q, K, V)={\rm Concat(head_1,\dots,head_h)}W^O \\ {\rm where\ head_i=Attention}(QW_i^Q,KW_i^K,VW_i^V)\)</span></p><p><span class="math inline">\(W_i^Q\in\mathbb{R}^{d_{model}\times d_k}, W_i^K\in\mathbb{R}^{d_{model}\times d_k}, W_i^V\in\mathbb{R}^{d_{model}\times d_v},W^O\in\mathbb{R}^{hd_v\times d_{model}}\)</span></p><p><span class="math inline">\(h=8,d_k=d_v=d_{model}/h=64\)</span></p><h5 id="position-wise-feed-forward-networks">Position-wise Feed-Forward Networks</h5><p>除了注意力子层外，encoder和decoder的每一层都包含一个全连接的前馈网络，分别作用于每个位置。两个线性变化中包含一个ReLU激活函数。 <span class="math display">\[{\rm FFN}(x)=\max(0, xW_1+b_1)W_2+b_2\]</span></p><p>虽然线性变换在不同位置上是相同的，但是他们使用的是不同的参数。</p><h5 id="embeddings-and-softmax">Embeddings and Softmax</h5><p>在两个embedding层和pre-softmax线性变换之间共享相同的权重矩阵。在embedding层中，将权重乘以<span class="math inline">\(\sqrt{d_{model}}\)</span></p><h5 id="position-encoding">Position Encoding</h5><p>由于模型不包含递归和卷积，为了使模型能够利用序列中的顺序信息，必须在序列中注入一些关于token相对或绝对位置的信息。为此，在encoder和decoder堆栈的底部将位置编码加到了输入中，位置编码的维度与input embedding的相同，均为<span class="math inline">\(d_{model}\)</span>，方便相加作为新的输入。计算位置编码的方式有很多，包括可学习的和固定的。</p><p>本文中，对不同的序列选择正弦和余弦函数来计算：</p><p><span class="math inline">\(PE_{(pos,2i)}=\sin(pos/10000^{2i/d_{model}})\)</span></p><p><span class="math inline">\(PE_{(pos,2i+1)}=\cos(pos/10000^{2i/d_{model}})\)</span></p><p>其中<span class="math inline">\(pos\)</span>是位置索引，<span class="math inline">\(i\)</span>是维数索引。位置编码的每个维度对应一条正弦曲线。波长呈<span class="math inline">\(2\pi\)</span>到<span class="math inline">\(100000\cdot2\pi\)</span>的几何级数。之所以选择这样的函数是因为：</p><ul><li>可以直接计算embedding不需要训练，减少了训练参数</li><li>允许模型将position embedding扩展到比训练过程中遇到的序列长度更长的序列</li></ul>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PyTorch学习</title>
    <link href="/2022/10/27/PyTorch%E5%AD%A6%E4%B9%A0/"/>
    <url>/2022/10/27/PyTorch%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<ul><li>torch.nn.init.xavier_uniform_(tensor,gain=1.0)</li></ul><p>网络训练过程中容易出现梯度消失或者梯度爆炸的情况，导致大部分反向传播得到的梯度不起作用或者起反作用。因此就需要一种合理的权重初始化方法，让计算过程中的数值分布更稳定。</p><p>Xavier初始化也称Glorot初始化，出自文章<a href="http://proceedings.mlr.press/v9/glorot10a">Understanding the difficulty of training deep feedforward neural networks</a>.</p><p>输出结果将从<span class="math inline">\(\mathcal{U}(-a,a)\)</span>中采样，</p><p><span class="math display">\[a=gain\times\sqrt{\frac{6}{fan\_in+fan\_out}}\]</span></p><p>类似的函数还有torch.init.xavier_normal，结果从<span class="math inline">\(\mathcal{N}(0,std^2)\)</span>中采样，</p><p><span class="math display">\[std=gain\times\sqrt{\frac{2}{fan\_in+fan\_out}}\]</span></p><style>.center {    width: auto;    display: table;    margin-left: auto;    margin-right: auto;}</style><div class="center"><table><thead><tr class="header"><th style="text-align: center;">nonlinearity</th><th style="text-align: center;">gain</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Linear/Identity</td><td style="text-align: center;"><span class="math inline">\(1\)</span></td></tr><tr class="even"><td style="text-align: center;">Conv{1,2,3}D</td><td style="text-align: center;"><span class="math inline">\(2\)</span></td></tr><tr class="odd"><td style="text-align: center;">Sigmoid</td><td style="text-align: center;"><span class="math inline">\(1\)</span></td></tr><tr class="even"><td style="text-align: center;">Tanh</td><td style="text-align: center;"><span class="math inline">\(\frac{5}{3}\)</span></td></tr><tr class="odd"><td style="text-align: center;">ReLu</td><td style="text-align: center;"><span class="math inline">\(\sqrt{2}\)</span></td></tr><tr class="even"><td style="text-align: center;">Leaky ReLU</td><td style="text-align: center;"><span class="math inline">\(\sqrt{\frac{2}{1+negative\_slope^2}}\)</span></td></tr><tr class="odd"><td style="text-align: center;">SELU</td><td style="text-align: center;"><span class="math inline">\(\frac{3}{4}\)</span></td></tr></tbody></table></div>]]></content>
    
    
    <categories>
      
      <category>PyTorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>PyTorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[论文阅读]DN-DETR: Accelerate DETR Training by Introducing Query DeNoising</title>
    <link href="/2022/07/09/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-DN-DETR-Accelerate-DETR-Training-by-Introducing-Query-DeNoising/"/>
    <url>/2022/07/09/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-DN-DETR-Accelerate-DETR-Training-by-Introducing-Query-DeNoising/</url>
    
    <content type="html"><![CDATA[<p>Carion等人创造性地将Transformer引入了目标检测领域，提出了DETR，掀起了CV业内一阵研究的热潮。DETR的提出是创造性的，但是也存在很多问题，例如收敛速度满，导致训练困难(要训练500个epochs才能达到理性的效果)。因而众多学者开始从不同方面入手，于对DETR进行优化，提出了 Dynamic DETR、DAB-DETR、Conditional DETR、Anchor DETR、Deformable DETR等DETR-like算法。</p><p>但是，鲜有人关注DETR中二分图匹配部分对训练过程中收敛速度的影响。有<a href="https://www.sciencedirect.com/science/article/pii/S0370157321000843">文章</a>已经证明DETR中使用的匈牙利算法并不是稳定匹配，<span class="math inline">\(cost\)</span>矩阵的微小变化都可能会导致匹配结果发生巨大的变化，进一步导致decoder queries中优化目标发生变化。</p><h3 id="衡量标准">衡量标准</h3><p>在DN-DETR中，作者提出了一种衡量二部匹配结果稳定性的标准：</p><p>对于每张训练图片，将Transformer decoders预测得到的物体定义为<span class="math inline">\(O^{i}={O_{0}^{i},O_{1}^{i},\dots,O_{N-1}^{i}}\)</span>其中<span class="math inline">\(i\)</span>表示第<span class="math inline">\(i\)</span>个epoch，<span class="math inline">\(N\)</span>为预测出的物体的数量。将ground truth中的物体定义为<span class="math inline">\(T={T_0,T_1,\dots,T_{M-1}}\)</span> 其中<span class="math inline">\(M\)</span>为ground truth中物体的数量。 在二部匹配后，计算一个索引向量<span class="math inline">\(V^i={V_0^i,V_1^i,\dots,V_{N-1}^i}\)</span>来存储第<span class="math inline">\(i\)</span>的epoch的匹配结果。</p><p><span class="math display">\[V_n^i=\left\{\begin{aligned}    m, &amp; \ if\ O_n^i\ matches \ T_m \\    -1, &amp; \ if\ O_n^i\ matches\ nothing\end{aligned}\right.\]</span></p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>Object Detection</tag>
      
      <tag>DETR-like</tag>
      
      <tag>transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
